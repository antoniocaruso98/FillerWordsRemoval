# Descrizione dataset

Il dataset di partenza è quello descritto in uno dei nostri paper di riferimento, [https://arxiv.org/pdf/2203.15135v1] (Filler Word Detection and Classification: A Dataset and Benchmark). In particolare il dataset completo si può trovare al seguente link: [https://podcastfillers.github.io/]

Il dataset in questione è costituito da una serie di **clip audio da 1 secondo** (circa 77.000) tratte da 199 episodi di podcast in lingua inglese, ciascuna **contenente uno e un solo evento** riconducibile a una tra **6 classi di filler words**:

- **Words**: parole confuse per via di disturbo audio, sovrapposizione di voci diverse, ecc.
- **Uh**: intercalare
- **Um**: intercalare
- **Breath**: eventi in cui si sente solo respiro del parlatore
- **Laughter**: eventi in cui si rileva una risata del parlatore
- **Music**: eventi in cui si rileva solo musica strumentale, ma non voce.

Il dataset è già suddiviso in **tre partizioni**, ovvero **train, test e validation**.

Tale dataset, tuttavia, nella sua forma originale, non era sufficiente per i nostri scopi, dunque abbiamo operato una serie di **trasformazioni/aggiunte sia sui dati, sia sulle etichette**:

1. Introduzione di una **nuova classe** che rappresentasse gli **esempi 'negativi'**, ovvero che rispecchiassero il normale parlato in **assenza di filler** words: **classe _Nonfiller_**. Per fare ciò, abbiamo sfruttato la presenza nel dataset, oltre che delle clip con le loro annotazioni, anche degli episodi completi e di annotazioni aggiuntive generate da un VAD (report periodico dell'attività vocale rilevata). Sono quindi state estratte **ulteriori 60K clip**, che hanno la caratteristica di **non avere intersezioni con intervalli contenenti filler words e silenzio** (clip ricercate considerando i soli intervalli in cui il VAD risultava attivo, secondo un'opportuna soglia). Le clip così ottenute sono state **ripartite in proporzione** tra i dataset di train, test e validation.
2. **Shift temporale di tutte le clip appartenenti a classi positive**: nel lavoro originale (di cui sopra), si prevedeva di effettuare esclusivamente classificazione delle clip, che pertanto sono state realizzate in maniera tale che il centro dell'evento da rilevare corrispondesse con 0.5s, ovvero il centro della clip stessa. Nel nostro caso, tuttavia, queste clip così generate si sono rivelate inadatte, poiché oltre alla classificazione, occorreva effettuare anche una **regressione** per prevedere le coordinate temporali del **bounding box** relativo all'evento, espresse in termini  di centro e ampiezza. Occorreva, dunque, effettuare uno shift di ogni clip, in modo tale che le coordinate del centro si potessero trovare, uniformemente, in ogni possibile posizione all'interno della clip, per permettere al modello di **generalizzare correttamente**. Per fare questo, abbiamo sfruttato le annotazioni presenti per rigenerare ogni singola clip positiva estraendola opportunamente dagli episodi completi, applicando un offset casuale (con distribuzione uniforme) rispetto all'intervallo riportato nell'annotazione.
3. **Modifica del formato delle etichette**: inizialmente i timestamp dei vari eventi era riportato nel formato **(start_s, end_s)**, che però era poco appropriato per le nostre esigenze, in quando era necessario poter prevedere anche, eventualmente, bounding box localizzati parzialmente al di fuori della finestra da 1 secondo. Per questo motivo abbiamo scelto di convertire questo formato di timestamp in **(center_t, delta_t)**, cosicché $start\_s = center\_t - delta\_t/2$ e $end\_s = center\_t + delta\_t/2$.

Il raggiungimento di questa formulazione finale del dataset, ha tuttavia richiesto **numerosi tentativi**, molti dei quali fallimentari:

1. Inizialmente, abbiamo pensato di ovviare al **problema delle clip centrate** decentrandole **on-the-fly** durante l'operazione di caricamento dei dati, semplicemente **traslando il dB-MEL-spectrogram** utilizzato come input per la rete convolutiva e inserendo, nella **parte 'mancante'** (ovvero quella rimasta vuota a seguito dello shift), **prima silenzio (riempimento con 0), poi rumore gaussiano**. Tuttavia questa strategia non si è rivelata efficace, perché **non permette al modello di generalizzare** correttamente sui dati forniti in fase di inferenza. Abbiamo quindi optato per rigenerare tutte le clip con offset desiderato.
2. La **presenza di eventuale silenzio** in dati reali, **in fase di inferenza**, costituiva un problema, in quanto nel dataset **non è presente una classe apposita** che descriva questo tipo di evento. Quindi abbiamo pensato di **escludere il silenzio dal training** assumendo che, in fase di inferenza, si utilizzi, **a monte del processamento del segnale tramite il modello**, un sistema di **eliminazione a priori degli intervalli di silenzio troppo lunghi** (esempio tramite utilizzo di un VAD o semplicemente tramite analisi della potenza del segnale audio - soluzione da noi adottata -).
3. Inizialmente, al fine di generare delle **clip della classe negativa**, avevamo scelto di **utilizzare un secondo dataset**, contenente dei file tratti da audiolibri (LibriSpeech ASR corpus: [https://www.openslr.org/12]). Il motivo dietro questa scelta può essere giustificato dal fatto che, generalmente, gli audiolibri sono caratterizzati da una dizione particolarmente accurata, tanto da poter fare l'assunzione che tali segnali audio fossero naturalmente privi di ogni possibile filler word. **Questa soluzione, tuttavia, si è rivelata inefficace in fase di training**, perché abbiamo osservato che l'utilizzo di due dataset con caratteristiche dei dati estremamente differenti inducevano semplicemente il modello a **operare una semplice separazione dei dataset** invece di concentrarsi sulle reali caratteristiche distintive di ogni classe. Questo fenomeno è apparso evidente nel momento in cui sono state analizzate le metriche di valutazione in presenza di trasformazioni per la data augmentation.
